#!/usr/bin/env python3
"""
Minimal SAE Feature Explanation Script

This script generates self-explanations for sparse autoencoder features using
activation steering with the Gemma-2-9B-IT model.
"""

import torch
import contextlib
from typing import Callable
from transformers.models.auto.modeling_auto import AutoModelForCausalLM
from transformers.models.auto.tokenization_auto import AutoTokenizer
from transformers.tokenization_utils import PreTrainedTokenizer
import einops

import interp_tools.saes.jumprelu_sae as jumprelu_sae
import interp_tools.model_utils as model_utils


def build_explanation_prompt(
    tokenizer: PreTrainedTokenizer,
    device: torch.device,
) -> tuple[torch.Tensor, int]:
    """
    Constructs a prompt for generating SAE feature explanations.

    Returns:
        A tuple containing the tokenized input IDs and the position of the 'X'
        placeholder where activations should be steered.
    """
    # Create chat format messages
    messages = [
        {
            "role": "user",
            "content": "Can you write explain to me what 'X' means? Format your final answer with <explanation>",
        },
    ]

    # Apply chat template
    formatted_input = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        continue_final_message=False,
    )
    print(f"Formatted input: {formatted_input}")

    # Find the position of the placeholder 'X'
    token_ids = tokenizer.encode(str(formatted_input), add_special_tokens=False)
    x_token_ids = tokenizer.encode("X", add_special_tokens=False)
    assert len(x_token_ids) == 1, "Expected to find 1 'X' token"
    x_token_id = x_token_ids[0]
    positions = [i for i, token_id in enumerate(token_ids) if token_id == x_token_id]

    print(f"Looking for X token ID: {x_token_id}")
    print(f"Found X at positions: {positions}")
    print(f"Total tokens: {len(token_ids)}")
    print(f"First 20 token IDs: {token_ids[:20]}")
    
    # Debug: decode around the X position
    if positions:
        pos = positions[0]
        start = max(0, pos - 5)
        end = min(len(token_ids), pos + 6)
        context_tokens = token_ids[start:end]
        context_text = tokenizer.decode(context_tokens)
        print(f"Context around X (pos {pos}): '{context_text}'")

    assert len(positions) == 1, (
        f"Expected to find 1 'X' placeholder, but found {len(positions)}. "
        f"Full prompt: {formatted_input}"
    )

    tokenized_input = tokenizer(
        str(formatted_input), return_tensors="pt", add_special_tokens=False
    ).to(device)

    return tokenized_input.input_ids, positions[0]


@contextlib.contextmanager
def add_hook(module: torch.nn.Module, hook: Callable):
    """Temporarily adds a forward hook to a model module."""
    handle = module.register_forward_hook(hook)
    try:
        yield
    finally:
        handle.remove()


def get_activation_steering_hook(
    vectors: list[list[torch.Tensor]],  # [B][K, d_model]  or [K, d_model] if B==1
    positions: list[list[int]],  # [B][K]
    steering_coefficient: float,
    device: torch.device,
    dtype: torch.dtype,
) -> Callable:
    """
    Returns a forward hook that *replaces* specified residual-stream activations
    during the initial prompt pass of `model.generate`.

    • vectors[b][k]  – feature vector to inject for batch b, slot k
    • positions[b][k]– token index (0-based, within prompt only)
    """

    # ---- pack Python lists → torch tensors once, outside the hook ----
    vec_BKD = torch.stack([torch.stack(v) for v in vectors])  # (B, K, d)
    pos_BK = torch.tensor(positions, dtype=torch.long)  # (B, K)

    B, K, d_model = vec_BKD.shape
    assert pos_BK.shape == (B, K)

    vec_BKD = vec_BKD.to(device, dtype)
    pos_BK = pos_BK.to(device)

    def hook_fn(module, _input, output):
        resid_BLD, *rest = output  # Gemma returns (resid, hidden_states, ...)
        L = resid_BLD.shape[1]

        # Only touch the *prompt* forward pass (sequence length > 1)
        if L <= 1:
            print(f"Skipping hook - sequence too short (L={L})")
            print(f"resid_BLD: {resid_BLD.shape}")
            return (resid_BLD, *rest)
        else:
            print("Hooking!")

        # Safety: make sure every position is inside current sequence
        if (pos_BK >= L).any():
            bad = pos_BK[pos_BK >= L].min().item()
            raise IndexError(f"position {bad} is out of bounds for length {L}")

        # ---- compute norms of original activations at the target slots ----
        batch_idx_B1 = torch.arange(B, device=device).unsqueeze(1)  # (B, 1) → (B, K)
        orig_BKD = resid_BLD[batch_idx_B1, pos_BK]  # (B, K, d)
        norms_BK1 = orig_BKD.norm(dim=-1, keepdim=True)  # (B, K, 1)

        # ---- build steered vectors ----
        steered_BKD = (
            torch.nn.functional.normalize(vec_BKD, dim=-1)
            * norms_BK1
            * steering_coefficient
        )  # (B, K, d)

        # ---- in-place replacement via advanced indexing ----
        resid_BLD[batch_idx_B1, pos_BK] = steered_BKD

        return (resid_BLD, *rest)

    return hook_fn


def main(
    sae_index: int = 0,
    steering_coefficient: float = 2.0,
    layer: int = 9,
    num_generations: int = 10,
):
    """
    Main function to generate SAE feature explanations.

    Args:
        sae_index: Index of the SAE feature to explain
        steering_coefficient: Strength of activation steering
        layer: Model layer to apply steering to
        num_generations: Number of explanations to generate
    """
    print(f"Generating {num_generations} explanations for SAE feature {sae_index}")
    print(f"Using steering coefficient: {steering_coefficient}, layer: {layer}")

    # Setup
    model_name = "google/gemma-2-9b-it"
    dtype = torch.bfloat16
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load model and tokenizer
    print("Loading model and tokenizer...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name, device_map="auto", torch_dtype=dtype
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Load SAE
    print(f"Loading SAE for layer {layer}...")
    sae_repo_id = "google/gemma-scope-9b-it-res"
    sae_filename = f"layer_{layer}/width_16k/average_l0_88/params.npz"

    sae = jumprelu_sae.load_gemma_scope_jumprelu_sae(
        repo_id=sae_repo_id,
        filename=sae_filename,
        layer=layer,
        model_name=model_name,
        device=device,
        dtype=dtype,
    )

    # Get the model submodule for the specified layer
    submodule = model_utils.get_submodule(model, layer)

    # Build prompt once
    orig_input_ids, x_position = build_explanation_prompt(tokenizer, device)
    orig_input_ids = orig_input_ids.squeeze()
    
    print(f"Original prompt length: {len(orig_input_ids)}")
    print(f"X position: {x_position}")
    print(f"Prompt: {tokenizer.decode(orig_input_ids)}")

    # Get feature vector (using decoder weights)
    feature_vector = sae.W_dec[sae_index]
    print(f"Feature vector shape: {feature_vector.shape}")

    # Prepare batch data for steering
    batch_steering_vectors = []
    batch_positions = []
    
    for i in range(num_generations):
        # Each batch item gets the same feature vector
        batch_steering_vectors.append([feature_vector])
        batch_positions.append([x_position])

    # Create batch input - repeat the same prompt for each generation
    input_ids_BL = einops.repeat(orig_input_ids, "L -> B L", B=num_generations)
    attn_mask_BL = torch.ones_like(input_ids_BL, dtype=torch.bool).to(device)

    tokenized_input = {
        "input_ids": input_ids_BL,
        "attention_mask": attn_mask_BL,
    }

    # Create steering hook
    hook_fn = get_activation_steering_hook(
        vectors=batch_steering_vectors,
        positions=batch_positions,
        steering_coefficient=steering_coefficient,
        device=device,
        dtype=dtype,
    )

    # Generation settings
    generation_kwargs = {
        "do_sample": True,
        "temperature": 1.0,
        "max_new_tokens": 200,
        "pad_token_id": tokenizer.eos_token_id,
    }

    # Generate all explanations at once
    print(f"\nGenerating {num_generations} explanations in batch...")
    print(f"Input shape: {tokenized_input['input_ids'].shape}")
    print(f"First few tokens: {tokenized_input['input_ids'][0, :10]}")
    
    with add_hook(submodule, hook_fn):
        output_ids = model.generate(**tokenized_input, **generation_kwargs)

    # Decode the generated tokens for each batch item
    explanations = []
    generated_tokens = output_ids[:, input_ids_BL.shape[1]:]
    
    for i in range(num_generations):
        decoded_output = tokenizer.decode(generated_tokens[i], skip_special_tokens=True)
        explanations.append(decoded_output)
        print(f"\nGeneration {i + 1}/{num_generations}:")
        print(decoded_output)
        print("-" * 80)

    return explanations


if __name__ == "__main__":
    # Example usage
    explanations = main(
        sae_index=0,
        steering_coefficient=20.0,
        layer=9,
        num_generations=10,
    )

    print(f"\nGenerated {len(explanations)} explanations total.")
