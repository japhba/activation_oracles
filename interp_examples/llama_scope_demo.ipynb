{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d528d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import einops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "import safetensors\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "class BaseSAE(nn.Module, ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_sae: int,\n",
    "        model_name: str,\n",
    "        hook_layer: int,\n",
    "        device: torch.device,\n",
    "        dtype: torch.dtype,\n",
    "        hook_name: str | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Required parameters\n",
    "        self.W_enc = nn.Parameter(torch.zeros(d_in, d_sae))\n",
    "        self.W_dec = nn.Parameter(torch.zeros(d_sae, d_in))\n",
    "\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_sae))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(d_in))\n",
    "\n",
    "        # Required attributes\n",
    "        self.device: torch.device = device\n",
    "        self.dtype: torch.dtype = dtype\n",
    "        self.hook_layer = hook_layer\n",
    "\n",
    "        hook_name = hook_name or f\"blocks.{hook_layer}.hook_resid_post\"\n",
    "        self.to(dtype=self.dtype, device=self.device)\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode(self, x: torch.Tensor):\n",
    "        \"\"\"Must be implemented by child classes\"\"\"\n",
    "        raise NotImplementedError(\"Encode method must be implemented by child classes\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def decode(self, feature_acts: torch.Tensor):\n",
    "        \"\"\"Must be implemented by child classes\"\"\"\n",
    "        raise NotImplementedError(\"Decode method must be implemented by child classes\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"Must be implemented by child classes\"\"\"\n",
    "        raise NotImplementedError(\"Forward method must be implemented by child classes\")\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        \"\"\"Handle device and dtype updates\"\"\"\n",
    "        super().to(*args, **kwargs)\n",
    "        device = kwargs.get(\"device\", None)\n",
    "        dtype = kwargs.get(\"dtype\", None)\n",
    "\n",
    "        if device:\n",
    "            self.device = device\n",
    "        if dtype:\n",
    "            self.dtype = dtype\n",
    "        return self\n",
    "\n",
    "\n",
    "class TopKSAE(BaseSAE):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_sae: int,\n",
    "        k: int,\n",
    "        model_name: str,\n",
    "        hook_layer: int,\n",
    "        device: torch.device,\n",
    "        dtype: torch.dtype,\n",
    "        use_threshold: bool = False,\n",
    "        hook_name: str | None = None,\n",
    "    ):\n",
    "        hook_name = hook_name or f\"blocks.{hook_layer}.hook_resid_post\"\n",
    "        super().__init__(d_in, d_sae, model_name, hook_layer, device, dtype, hook_name)\n",
    "\n",
    "        assert isinstance(k, int) and k > 0\n",
    "        self.register_buffer(\"k\", torch.tensor(k, dtype=torch.int, device=device))\n",
    "        self.d_sae = d_sae\n",
    "        self.d_in = d_in\n",
    "        self.pre_encoder_bias = False\n",
    "\n",
    "        self.use_threshold = use_threshold\n",
    "        if use_threshold:\n",
    "            # Optional global threshold to use during inference. Must be positive.\n",
    "            self.register_buffer(\n",
    "                \"threshold\", torch.tensor(-1.0, dtype=dtype, device=device)\n",
    "            )\n",
    "\n",
    "    def encode(self, x: torch.Tensor):\n",
    "        \"\"\"Note: x can be either shape (B, F) or (B, L, F)\"\"\"\n",
    "        if self.pre_encoder_bias:\n",
    "            x = x - self.b_dec\n",
    "\n",
    "        post_relu_feat_acts_BF = nn.functional.relu(x @ self.W_enc + self.b_enc)\n",
    "\n",
    "        if self.use_threshold:\n",
    "            if self.threshold < 0:\n",
    "                raise ValueError(\n",
    "                    \"Threshold is not set. The threshold must be set to use it during inference\"\n",
    "                )\n",
    "            encoded_acts_BF = post_relu_feat_acts_BF * (\n",
    "                post_relu_feat_acts_BF > self.threshold\n",
    "            )\n",
    "            return encoded_acts_BF\n",
    "\n",
    "        post_topk = post_relu_feat_acts_BF.topk(self.k, sorted=False, dim=-1)\n",
    "\n",
    "        tops_acts_BK = post_topk.values\n",
    "        top_indices_BK = post_topk.indices\n",
    "\n",
    "        buffer_BF = torch.zeros_like(post_relu_feat_acts_BF)\n",
    "        encoded_acts_BF = buffer_BF.scatter_(\n",
    "            dim=-1, index=top_indices_BK, src=tops_acts_BK\n",
    "        )\n",
    "        return encoded_acts_BF\n",
    "\n",
    "    def decode(self, feature_acts: torch.Tensor):\n",
    "        return (feature_acts @ self.W_dec) + self.b_dec\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.encode(x)\n",
    "        recon = self.decode(x)\n",
    "        return recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0674d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_llama_scope_topk_sae(\n",
    "    model_name: str,\n",
    "    device: torch.device,\n",
    "    dtype: torch.dtype,\n",
    "    layer: int,\n",
    "    expansion_factor: int,\n",
    ") -> TopKSAE:\n",
    "    repo_id = f\"fnlp/Llama3_1-8B-Base-LXR-{expansion_factor}x\"\n",
    "    config_filename = f\"Llama3_1-8B-Base-L{layer}R-{expansion_factor}x/hyperparams.json\"\n",
    "    filename = (\n",
    "        f\"Llama3_1-8B-Base-L{layer}R-{expansion_factor}x/checkpoints/final.safetensors\"\n",
    "    )\n",
    "\n",
    "    path_to_params = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=filename,\n",
    "        force_download=False,\n",
    "        local_dir=\"downloaded_saes\",\n",
    "    )\n",
    "\n",
    "    path_to_config = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=config_filename,\n",
    "        force_download=False,\n",
    "        local_dir=\"downloaded_saes\",\n",
    "    )\n",
    "\n",
    "    with open(path_to_config) as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    threshold = config[\"jump_relu_threshold\"]\n",
    "    k = config[\"top_k\"]\n",
    "\n",
    "    pt_params = safetensors.torch.load_file(path_to_params)\n",
    "\n",
    "    key_mapping = {\n",
    "        \"encoder.weight\": \"W_enc\",\n",
    "        \"decoder.weight\": \"W_dec\",\n",
    "        \"encoder.bias\": \"b_enc\",\n",
    "        \"decoder.bias\": \"b_dec\",\n",
    "    }\n",
    "\n",
    "    renamed_params = {key_mapping.get(k, k): v for k, v in pt_params.items()}\n",
    "    renamed_params[\"W_enc\"] = renamed_params[\"W_enc\"].T\n",
    "    renamed_params[\"W_dec\"] = renamed_params[\"W_dec\"].T\n",
    "    renamed_params[\"k\"] = torch.tensor(k, dtype=torch.int, device=device)\n",
    "    renamed_params[\"threshold\"] = torch.tensor(threshold, dtype=dtype, device=device)\n",
    "\n",
    "    print(renamed_params.keys())\n",
    "\n",
    "    d_in = renamed_params[\"b_dec\"].shape[0]\n",
    "    d_sae = renamed_params[\"b_enc\"].shape[0]\n",
    "\n",
    "    assert d_in <= d_sae, \"d_in must be less than or equal to d_sae\"\n",
    "\n",
    "    sae = TopKSAE(\n",
    "        d_in=d_in,\n",
    "        d_sae=d_sae,\n",
    "        k=k,\n",
    "        model_name=model_name,\n",
    "        hook_layer=layer,\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "        use_threshold=True,\n",
    "    )\n",
    "\n",
    "    sae.load_state_dict(renamed_params)\n",
    "    sae.to(device=device, dtype=dtype)\n",
    "\n",
    "    assert sae.W_enc.shape == (d_in, d_sae)\n",
    "    assert sae.W_dec.shape == (d_sae, d_in)\n",
    "\n",
    "    # https://github.com/OpenMOSS/Language-Model-SAEs/blob/25180e32e82176924b62ab30a75fffd234260a9e/src/lm_saes/sae.py#L172\n",
    "    # openmoss scaling strategy\n",
    "    dataset_average_activation_norm = config[\"dataset_average_activation_norm\"]\n",
    "    input_norm_factor = sae.d_in**0.5 / dataset_average_activation_norm[\"in\"]\n",
    "    sae.b_enc.data /= input_norm_factor\n",
    "    sae.b_dec.data /= input_norm_factor\n",
    "\n",
    "    return sae\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "835d0538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['b_dec', 'W_dec', 'b_enc', 'W_enc', 'k', 'threshold'])\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "# note: will also work for Instruct models\n",
    "layer = 9\n",
    "expansion_factor = 8\n",
    "\n",
    "sae = load_llama_scope_topk_sae(\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    "    layer=layer,\n",
    "    expansion_factor=expansion_factor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3eb45f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.04it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a440be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodule = model.model.layers[layer]\n",
    "\n",
    "test_input = \"The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science\"\n",
    "test_input = tokenizer(test_input, return_tensors=\"pt\", add_special_tokens=True).to(\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0f2e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopException(Exception):\n",
    "    \"\"\"Custom exception for stopping model forward pass early.\"\"\"\n",
    "    pass\n",
    "\n",
    "def collect_activations(\n",
    "    model: AutoModelForCausalLM,\n",
    "    submodule: torch.nn.Module,\n",
    "    inputs_BL: dict[str, torch.Tensor],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Registers a forward hook on the submodule to capture the residual (or hidden)\n",
    "    activations. We then raise an EarlyStopException to skip unneeded computations.\n",
    "\n",
    "    Args:\n",
    "        model: The model to run.\n",
    "        submodule: The submodule to hook into.\n",
    "        inputs_BL: The inputs to the model.\n",
    "    \"\"\"\n",
    "    activations_BLD = None\n",
    "\n",
    "    def gather_target_act_hook(module, inputs, outputs):\n",
    "        nonlocal activations_BLD\n",
    "        # For many models, the submodule outputs are a tuple or a single tensor:\n",
    "        # If \"outputs\" is a tuple, pick the relevant item:\n",
    "        #   e.g. if your layer returns (hidden, something_else), you'd do outputs[0]\n",
    "        # Otherwise just do outputs\n",
    "        if isinstance(outputs, tuple):\n",
    "            activations_BLD = outputs[0]\n",
    "        else:\n",
    "            activations_BLD = outputs\n",
    "\n",
    "        raise EarlyStopException(\"Early stopping after capturing activations\")\n",
    "\n",
    "    handle = submodule.register_forward_hook(gather_target_act_hook)\n",
    "\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs_BL)\n",
    "    except EarlyStopException:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during forward pass: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        handle.remove()\n",
    "\n",
    "    return activations_BLD\n",
    "\n",
    "acts_BLD = collect_activations(model, model.model.layers[layer], test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "504781e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average l0: 32.45161056518555\n",
      "variance explained: 0.626794695854187\n"
     ]
    }
   ],
   "source": [
    "encoded_acts_BLF = sae.encode(acts_BLD)\n",
    "\n",
    "decoded_acts_BLD = sae.decode(encoded_acts_BLF)\n",
    "\n",
    "flattened_acts_BD = einops.rearrange(acts_BLD, \"b l d -> (b l) d\")\n",
    "reconstructed_acts_BD = sae(flattened_acts_BD)\n",
    "# match flattened_acts with decoded_acts\n",
    "reconstructed_acts_BLD = reconstructed_acts_BD.reshape(acts_BLD.shape)\n",
    "\n",
    "assert torch.allclose(reconstructed_acts_BLD, decoded_acts_BLD)\n",
    "\n",
    "l0 = (encoded_acts_BLF[:, 1:] > 0).float().sum(-1).detach()\n",
    "print(f\"average l0: {l0.mean().item()}\")\n",
    "\n",
    "variance_explained = 1 - torch.mean(\n",
    "    (reconstructed_acts_BLD[:, 1:] - acts_BLD[:, 1:].to(torch.float32)) ** 2\n",
    ") / (acts_BLD[:, 1:].to(torch.float32).var())\n",
    "\n",
    "print(f\"variance explained: {variance_explained.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c131f759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
